{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226ece7-9110-4643-86a5-ab5215b49db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "# pip install tqdm\n",
    "# pip install transformers\n",
    "# pip install torch\n",
    "# pip install morfeusz2\n",
    "# pip install spacy\n",
    "# pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13457008-87d3-4182-a7b1-162f02daad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import goose3\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import morfeusz2\n",
    "import spacy\n",
    "import subprocess\n",
    "import sys\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"pl_core_news_lg\"])\n",
    "nlp_spacy = spacy.load(\"pl_core_news_lg\")\n",
    "\n",
    "import requests\n",
    "from goose3 import Goose\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"pczarnik/herbert-base-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "nlp_herbert_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "import stanza\n",
    "stanza.download('pl')\n",
    "nlp_stanza = stanza.Pipeline('pl', processors='tokenize,ner')\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850c0bf-ef26-46b2-8d75-ce1e650baee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad559f4-4c33-4293-b95f-8823da87aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH ARTICLES\n",
    "\n",
    "# df = pd.read_json('data/articles_koryta.jsonl', lines=True)\n",
    "# g = Goose()\n",
    "\n",
    "# def fetch_article(url):\n",
    "#     try:\n",
    "#         r = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=10)\n",
    "#         r.raise_for_status()\n",
    "#         r.encoding = r.apparent_encoding\n",
    "#         article = g.extract(raw_html=r.text)\n",
    "#         return {\n",
    "#             \"title\": article.title,\n",
    "#             \"text\": article.cleaned_text,\n",
    "#             \"meta_description\": article.meta_description,\n",
    "#         }\n",
    "#     except:\n",
    "#         return {\n",
    "#             \"title\": None,\n",
    "#             \"text\": None,\n",
    "#             \"meta_description\": None\n",
    "#         }\n",
    "\n",
    "# df_temp = df['url'].progress_apply(fetch_article)\n",
    "# df[['title','text','meta_description']] = df_temp.apply(pd.Series)\n",
    "# df.to_json('data/scraped_articles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc9bc6-91ea-498c-b582-8c36551b2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS\n",
    "def extract_entities_herbert(text):\n",
    "\n",
    "    ner_output = nlp_herbert_ner(text)\n",
    "    entities = {\n",
    "        'PER': [],\n",
    "        'LOC': [],\n",
    "        'ORG': []\n",
    "    }\n",
    "\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "\n",
    "    for token in ner_output:\n",
    "        tag = token['entity']\n",
    "        word = token['word'].replace('</w>', ' ')\n",
    "\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity and current_type:\n",
    "                entities[current_type].append(''.join(current_entity))\n",
    "            current_type = tag[2:]\n",
    "            current_entity = [word]\n",
    "\n",
    "        elif tag.startswith('I-') and current_type == tag[2:]:\n",
    "            current_entity.append(word)\n",
    "\n",
    "        else:\n",
    "            if current_entity and current_type:\n",
    "                entities[current_type].append(''.join(current_entity))\n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "\n",
    "    if current_entity and current_type:\n",
    "        entities[current_type].append(''.join(current_entity))\n",
    "\n",
    "    return entities\n",
    "\n",
    "def fix_spacing_full_names(full_name):\n",
    "    # tokenize string\n",
    "    tokens = re.findall(r'\\b\\w+\\b', full_name)\n",
    "\n",
    "    if not tokens:\n",
    "        return entity_str\n",
    "\n",
    "    result = tokens[0]\n",
    "    for token in tokens[1:]:\n",
    "        if token[0].isupper():\n",
    "            result += ' ' + token\n",
    "        else:\n",
    "            result += token \n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "# # DENOMINATIVE FULL NAMES\n",
    "# def name_to_nominative_morf(full_name):\n",
    "#     morfeusz = morfeusz2.Morfeusz()\n",
    "#     words = full_name.split()\n",
    "#     nominative_words = []\n",
    "\n",
    "#     for word in words:\n",
    "#         analyses = morfeusz.analyse(word)\n",
    "#         selected_lemma = None\n",
    "\n",
    "#         for _, _, morph in analyses:\n",
    "#             lemma = morph[0]\n",
    "#             meanings = morph[3]  # lista znaczeń\n",
    "\n",
    "#             if 'imię' in meanings or 'nazwisko' in meanings:\n",
    "#                 selected_lemma = lemma\n",
    "#                 break  # bierzemy pierwszą napotkaną formę z imieniem lub nazwiskiem\n",
    "\n",
    "#         if selected_lemma:\n",
    "#             nominative_words.append(selected_lemma)\n",
    "#         else:\n",
    "#             # fallback - pierwsza lemma bez względu na znaczenia\n",
    "#             nominative_words.append(analyses[0][2][0])\n",
    "\n",
    "#     return ' '.join(nominative_words)\n",
    "\n",
    "def lemmatize_name_spacy(name):\n",
    "    doc = nlp_spacy(name)\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def extract_stanza(document, pos_type):\n",
    "    #persName, orgName, placeName\n",
    "    findings = []\n",
    "    current = []\n",
    "\n",
    "    for sentence in document.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            ner_tag = token.ner\n",
    "            text = token.text\n",
    "\n",
    "            if ner_tag == f'B-{pos_type}':\n",
    "                current.append(text)\n",
    "            elif ner_tag == f'I-{pos_type}':\n",
    "                current.append(text)\n",
    "            elif ner_tag == f'E-{pos_type}':\n",
    "                current.append(text)\n",
    "                full = ' '.join(current)\n",
    "                findings.append(full)\n",
    "                current_name = []\n",
    "            elif ner_tag == f'S-{pos_type}':\n",
    "                findings.append(text)\n",
    "\n",
    "    return findings\n",
    "\n",
    "def texts_similarity(s1, s2):\n",
    "    score = fuzz.ratio(s1, s2)\n",
    "    return score / 100.0\n",
    "\n",
    "def max_with_default_zero(lst):\n",
    "    return max(lst) if lst else 0\n",
    "\n",
    "\n",
    "# double metaphone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1f3ee-d32d-4a1b-8d54-65786ec556f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/scraped_articles.json')\n",
    "df = df[df['text']!='']\n",
    "df = df[~(df['text'].isnull())]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee617a-3fd1-4ce6-9e7e-ece72aa1a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HERBERT\n",
    "df['herbert_ner_entities'] = df['text'].progress_apply(extract_entities_herbert)\n",
    "df[['PER_herbert','LOC_herbert','ORG_herbert']] = df['herbert_ner_entities'].apply(pd.Series)\n",
    "df['PER_herbert'] = df['PER_herbert'].apply(lambda x: [fix_spacing_full_names(full_name) for full_name in x]).apply(lambda x: [lemmatize_name_spacy(full_name) for full_name in x])\n",
    "df['PER_match_herbert'] = df.apply(lambda x: max_with_default_zero([texts_similarity(x['mentioned_person'], elem) for elem in x['PER_herbert']]),axis=1) > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b39c3-d8ea-4dad-8b85-d0eda49fad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STANZA\n",
    "df['stanza_entities'] = df['text'].progress_apply(nlp_stanza)\n",
    "df['PER_stanza'] = df['stanza_entities'].progress_apply(extract_stanza, args=('persName',)).apply(lambda x: [fix_spacing_full_names(elem) for elem in x]).apply(lambda x: [lemmatize_name_spacy(elem) for elem in x])\n",
    "df['ORG_stanza'] = df['stanza_entities'].progress_apply(extract_stanza, args=('orgName',))\n",
    "df['LOC_stanza'] = df['stanza_entities'].progress_apply(extract_stanza, args=('placeName',))\n",
    "df['PER_match_stanza'] = df.apply(lambda x: max_with_default_zero([texts_similarity(x['mentioned_person'], elem) for elem in x['PER_stanza']]),axis=1) > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d4b7f-d48a-40e8-b40b-09166d1373b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda29fb-e4ea-4147-92d3-c4fa316a7d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb3052-3081-4581-ad5b-5636bf065d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d17adf-ae39-4b78-b846-e6d4d453717d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e1c91-9063-4eb0-a803-966b9ab610c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 NER",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
